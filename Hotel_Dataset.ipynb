{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9106d218-de89-4805-8d01-c57fdebc0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5db8401-bd4d-4554-88db-fb8bd0849a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/Hotel_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0e235-771d-4185-a25f-ea566f9e5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f75841-11c3-4960-afe1-309a90a1bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d9227b-d5c6-4701-9fb8-daa796249231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Negative_Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da639479-cb12-4221-846f-339eecb5b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Positive_Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3248d87e-e84a-41a3-92bf-b82b9217fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Negative_Review'] = df['Negative_Review'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a4eb6a5-9690-4d4e-9804-ec457c673099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Positive_Review'] = df['Positive_Review'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4d2b6-a94e-41d9-b2df-2ff7631bf5a6",
   "metadata": {},
   "source": [
    "# Removal Of Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ddc43b-41c1-452a-b267-635fa6a33145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hassa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4d53e3-befb-48c8-8aa0-bdbc36894459",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "251eecb5-df1e-4040-9ddd-9365aed59530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_Negative_Review'] = df['Negative_Review'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54820a88-08f2-4847-85e8-a9ec05c21c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_Postive_Review'] = df['Positive_Review'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3308c17-093b-4960-9f7d-5257a07e11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7b725-71a3-44ff-8b23-6f2fc71f33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_Negative_Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53102a-78ba-44ab-87c4-762bc233fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_Postive_Review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a977b-a1e4-4ed9-9e6a-167c141f2533",
   "metadata": {},
   "source": [
    "# Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba67b44-4361-4135-b8e5-a0963f4e4735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('room', 176026),\n",
       " ('negative', 129447),\n",
       " ('hotel', 74709),\n",
       " ('breakfast', 58478),\n",
       " ('small', 49880),\n",
       " ('staff', 39512),\n",
       " ('nothing', 38768),\n",
       " ('rooms', 34802),\n",
       " ('would', 32291),\n",
       " ('could', 32079),\n",
       " ('bed', 29828),\n",
       " ('one', 28095),\n",
       " ('bit', 27546),\n",
       " ('bathroom', 26585),\n",
       " ('night', 24063),\n",
       " ('little', 22536),\n",
       " ('like', 22445),\n",
       " ('shower', 21290),\n",
       " ('good', 20821),\n",
       " ('us', 20225)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collection\n",
    "from collections import Counter\n",
    "word_count = Counter()\n",
    "for text in df['Clean_Negative_Review']:\n",
    "    for word in text.split():\n",
    "        word_count[word] += 1\n",
    "\n",
    "\n",
    "word_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6338e4c0-03fa-42fb-a798-97891ad902b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQUENT_WORDS = set(word for (word, wc) in word_count.most_common(3))\n",
    "def remove_freq_word(text):\n",
    "    return \" \".join([word for word in text.split() if word not in FREQUENT_WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e05267-8b82-4642-a05a-5d82ba815853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_specific_values(text):\n",
    "    import re\n",
    "    # Replace the specific values with an empty string\n",
    "    return re.sub(r'\\b0[0-9a-zA-Z]*\\b', '', text).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "134513a2-f24a-4734-a44d-01482e4942ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'text' column\n",
    "df['Clean_Negative_Review'] = df['Clean_Negative_Review'].apply(remove_specific_values)\n",
    "df['Clean_Postive_Review'] = df['Clean_Postive_Review'].apply(remove_specific_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341db92-a6d4-4556-bd56-81cf13225cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ba5f46a-dbaf-4c48-96be-5219ebc768d5",
   "metadata": {},
   "source": [
    "# Rare Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51cc1436-0952-4652-8f36-33116c10c297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aerothermal',\n",
       " 'beda',\n",
       " 'cinde',\n",
       " 'complintary',\n",
       " 'hommy',\n",
       " 'immpossible',\n",
       " 'lau',\n",
       " 'tioned',\n",
       " 'wardroom'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RARE_WORDS = set(word for (word, wc) in word_count.most_common()[:-10:-1])\n",
    "RARE_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2f227-02b0-4574-a388-7f73818efd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c2e0f-dda6-495e-b4ed-70a79833fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stem_word(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d57e41-00a0-45af-bd16-f560c0245e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stemed_Negative_Review'] = df['Clean_Negative_Review'].apply(lambda x: stem_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3fb3a6-39dd-497c-9bae-ee5a9e35eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stemed_Negative_Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57372efd-2629-4257-b800-9a1e031aaea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='stemed_Review', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca236cc-cc52-4274-a528-6ef36746d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stemed_Postive_Review'] = df['Clean_Postive_Review'].apply(lambda x: stem_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96020d-ed6a-40e7-8b6b-f582ff38f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "lematizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    pos_text = pos_tag(text.split())\n",
    "    return \" \".join([lematizer.lemmatize(word,wordnet_map.get(pos[0],wordnet.NOUN)) for word, pos in pos_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e961b-7c19-4fa1-9c68-dd7be93d291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382720f4-283b-4658-a314-df8ce441c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa12be-7813-42e6-a81c-17e19799b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lemmatized_Negative_Review'] = df['Clean_Negative_Review'].apply(lambda x: lemmatize_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad543f4d-1750-437c-a775-3ecbb6cea2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lemmatized_Positive_Review'] = df['Clean_Postive_Review'].apply(lambda x: lemmatize_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ba330-e559-4fb7-a014-ba8f31c5548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a350c-535a-4b88-984b-36812c9dfb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset/clean_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edddee3-0b29-4100-acc3-d5265cda2b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52f443-b862-4924-88d9-a2c5ea2c115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lemmatized_Positive_Review'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76440edf-58a8-40bf-888a-f291886d0833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2cbb07-a51e-41ff-a802-b0b918e3ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Lemmatized_Positive_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ffec9-d5eb-4510-9ddd-3d7cb1e7772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1204c4ae-466d-4f90-8f49-2b335a4cd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "# nltk.download('punkt')\n",
    "# df['tokens_postive_review'] = df['Lemmatized_Positive_Review'].apply(nltk.word_tokenize)\n",
    "\n",
    "# df['tokens_str'] = df['tokens_postive_review'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(df['Clean_Negative_Review'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc46de3-7824-419c-bb62-ef68408e180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f3204-3372-49d8-93c8-7ecd4fbe066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_tfidf_sample = tfidf_matrix[:5].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a1938-bdc4-4d29-9c81-4645126c587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(dense_tfidf_sample, columns=count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6772a68-729f-415c-a316-299306cd7b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7219a65f-fbce-4fb1-8713-8fa3862f4712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '100', '1000', ..., 'zzzz', 'zzzzzzz', 'zzzzzzzzzzzz'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d5deb3c-966b-4f38-b4d2-cee98d099162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant features and their mean TF-IDF scores:\n",
      "negative: 0.24843576507416015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Calculate the significance of each feature (column)\n",
    "# We use the mean tf-idf score across all documents\n",
    "feature_significance = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "\n",
    "# Define a threshold to filter out less significant features\n",
    "threshold = 0.1\n",
    "significant_indices = np.where(feature_significance > threshold)[0]\n",
    "\n",
    "# Filter the TF-IDF matrix to keep only significant features\n",
    "filtered_tfidf_matrix = tfidf_matrix[:, significant_indices]\n",
    "\n",
    "# Get the feature names for the significant features\n",
    "significant_feature_names = np.array(count_vectorizer.get_feature_names_out())[significant_indices]\n",
    "\n",
    "# If you need the filtered matrix as a DataFrame\n",
    "# import pandas as pd\n",
    "# filtered_tfidf_df = pd.DataFrame(filtered_tfidf_matrix.toarray(), columns=significant_feature_names)\n",
    "\n",
    "# Output the significant feature names and their mean scores for verification\n",
    "print(\"Significant features and their mean TF-IDF scores:\")\n",
    "for feature, score in zip(significant_feature_names, feature_significance[significant_indices]):\n",
    "    print(f\"{feature}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "064e09c2-be80-42ee-a692-9942243e1979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<515738x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 129431 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a8206-3c23-4e79-8810-731cfaf3853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_clean_negative_review = tfidf_vec.fit_transform(df['Clean_Negative_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ac708-bb24-4361-a246-77012610cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    " # to print vocabulary\n",
    "print(\"Vocabulary:\\n\", tfidf_vec.vocabulary_)\n",
    "print(\"\\n\\nFeatures:\\n\", tfidf_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cda48c-8d89-431c-923d-418334b8cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_clean_negative_review.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f61422-fcb9-468a-b0cb-cf0bd443eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined'] = df['Clean_Negative_Review'] + ' ' + df['Clean_Postive_Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3bbac-8e2e-49e9-984f-7b5774037a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0da74-7da4-4682-9047-a6adbbec625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = vectorizer.fit_transform(df['Clean_Negative_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdb7ca-0265-44da-8c1d-0605cf0a6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray().astype('float32'), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285788d-9b3e-4bdd-b618-d0a4aea45059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output sparse matrix information\n",
    "print(f\"Sparse Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Sparse Matrix Type: {type(tfidf_matrix)}\")\n",
    "\n",
    "# If you need to access specific rows or columns of the TF-IDF matrix\n",
    "row_index = 0\n",
    "column_index = 0\n",
    "print(f\"TF-IDF value at row {row_index}, column {column_index}: {tfidf_matrix[row_index, column_index]}\")\n",
    "\n",
    "# Save the sparse matrix to disk if needed\n",
    "from scipy import sparse\n",
    "sparse.save_npz('tfidf_matrix.npz', tfidf_matrix)\n",
    "\n",
    "# To load the matrix back\n",
    "loaded_tfidf_matrix = sparse.load_npz('tfidf_matrix.npz')\n",
    "print(f\"Loaded Sparse Matrix Shape: {loaded_tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d0de6-84c2-45e0-9547-490381b3a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz, load_npz, csr_matrix\n",
    "\n",
    "# Define the function to convert sparse matrix to DataFrame in chunks\n",
    "def sparse_matrix_to_dataframe(sparse_matrix: csr_matrix, chunk_size: int = 10000) -> pd.DataFrame:\n",
    "    n_rows, n_cols = sparse_matrix.shape\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for start_row in range(0, n_rows, chunk_size):\n",
    "        end_row = min(start_row + chunk_size, n_rows)\n",
    "        chunk = sparse_matrix[start_row:end_row].toarray().astype('float32')\n",
    "        chunk_df = pd.DataFrame(chunk, columns=feature_names)\n",
    "        df = pd.concat([df, chunk_df], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert the loaded sparse matrix to a DataFrame\n",
    "tfidf_df = sparse_matrix_to_dataframe(loaded_tfidf_matrix, chunk_size=10000)\n",
    "\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca244c6-cb6b-4d45-8106-0fd343e584b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
